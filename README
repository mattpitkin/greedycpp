Overview
--------

A fast, scalable and easy-to-use parallelization of the greedy algorithm
for building an application-specific reduced basis. 

In many cases, for example when implimented on a computer and using
the Euclidean inner product, the greedy algorithm is QR with column pivoting.
And so this code may also be used to compute QR decompositions of a large
matrix.

For details on the code's scaling and QR-based model reduction:

[1] Harbir Antil, Dangxing Chen, and Scott Field. "QR-based model 
    reduction for large problems". In preperation. 


Requirements: libconfig, gsl and MPI (see below to build with mpi disabled).

Code's coordinates: https://sfield83@bitbucket.org/sfield83/greedycpp.git


Quick start
-----------

         -- serial mode --
1) edit greedy.cpp by commenting out the line "#define COMPILE_WITH_MPI"
2) run 'make greedy'
3) run './greedy PARAM_FILE.cfg'


         -- mpi --
1) run 'make'
2) mpirun -np NUM_PROCS ./greedympi PARAM_FILE.cfg


Running example 1
-----------------
Lets run an example problem using the configuration file provided in example1/

>>> mpirun -np 4 ./ greedympi examples/test1.cfg

A directory called 'my_output_test1' contains the output. Check that 
the output is whats expected by doing

>>> python regression_test.py examples/output_test1 my_output_test1/

This should report on the differences as well as generating a summary 
html file. Differences, if there any, should be machine precision. 

Next, lets modify test1.cfg. The relevant variables should be changed to 

load_from_file = false;
params_num  = [30,30];
params_low  = [1.0,1.0];
params_high = [3.0,3.0];

These settings will, to the first 10 decimal places or so, reproduce the same
training set samplings as that give from file "examples/test1_TS_Points.txt". 
Next, do 

>>> mpirun -np 4 ./ greedympi example1/test1.cfg

You should find minor differences similar to:

>>> python regression_test.py example1/ example1_output/

Mismatches in the following files: 
==>      ApproxErrors.txt : Maximum difference =   9.00391e-14 on line 261
==>      GreedyPoints.txt : Maximum difference =   1.02141e-14 on line 264
==>        Basis_real.txt : Maximum difference =   1.53470e-07 on line 270
==>        Basis_imag.txt : Maximum difference =   1.27563e-07 on line 270
==>            R_real.txt : Maximum difference =   3.39999e-13 on line 270
==>            R_imag.txt : Maximum difference =   4.36700e-13 on line 270


While differences of 1e-07 might seem large, notice that the 269th 
basis is the last one, and its contribution to the approximation 
is on the order of tol = 1e-12


Running on SuperMike2 with MPI
------------------------------
0) add the following to ~/.soft:
@default
+Intel-13.0.0
@default
+gsl-1.15-Intel-13.0.0
@default
+openmpi-1.6.3-Intel-13.0.0
@default
+fftw-3.3.3-Intel-13.0.

and run 'resoft'

Using the lsu makefile

2) run 'make'
3) open mpiGreedyQsub and set number of nodes.
4) run 'qsub mpiGreedyQsub' (this will print a jobID to screen)
5) monitor job with qstat jobID


Troubleshooting
---------------

1) If gsl not found run

gsl-config --cflags --libs-without-cblas

and the output says what must be added to g++ to compile and link.

2) On Macs: Install 'libconfig-hr' from macports instead of the package 
   'libconfig' which is just the c-version

3) If ld cannot link to libconfig++ (problem on Supermike2), 
   build locally after getting libconfig

>>> wget http://www.hyperrealm.com/libconfig/libconfig-1.4.9.tar.gz

Then update LD_LIBRARY_PATH to reflect the location of the newly created
libconfig++ shared library.


Problem motivation and typical application
------------------------------------------

Given a parameterized model h(x;p), whose physical variable is x and
parameterization p. 

From a collection of model evaluations at known at a discrete training set
of parameter p_i and physical x_j points, we have a training space

   { h(x_j;p_i) }

This training space fills up a matrix A such that the i-th row is the model
evaluation h(\vec{x};p_i). 

The routine greedy.cpp finds a low-rank approximation of A by a 
pivoted QR (aka greedy) decomposition. The output are a collection 
of p_i (greedy parameter values), the orthonormal basis, and an 
R-matrix such that 

   A = R * Q 

where the rows of Q are precisely the basis.


Acknowledgements
----------------
Priscilla Canizares, Collin Capano, Michael PÃ¼rrer, and Rory Smith for 
careful error reporting, code improvements and testing of early versions of 
the code. Peter Diener for performance optimizations.
